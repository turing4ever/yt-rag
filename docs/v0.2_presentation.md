# yt-rag: A RAG Pipeline for Your Favorite YouTube Channels

**Version 0.2.0** | December 11, 2025

## A Technical Deep-Dive

---

# Agenda

1. [**What is RAG?**](#part-1-what-is-rag) - Core concepts for newcomers
2. [**System Architecture**](#part-2-system-architecture) - How yt-rag works
3. [**The Data Pipeline**](#part-3-the-data-pipeline) - From YouTube to searchable vectors
4. [**Hybrid Search**](#part-4-hybrid-search) - Combining semantic and keyword matching
5. [**Design Decisions & Tradeoffs**](#part-5-design-decisions--tradeoffs) - What we learned
6. [**Benchmarking**](#part-6-benchmarking) - Testing and evaluation results

[Appendix](#appendix) | [Key Takeaways](#key-takeaways) | [Try It Yourself](#try-it-yourself-v020)

---

# Part 1: What is RAG?

---

## The Problem: LLMs Don't Know Your Data

Ask ChatGPT about a specific YouTube video:

```
User: "What did SavageGeese say about the Toyota Crown's ride quality?"

ChatGPT: "I don't have access to specific YouTube videos or their
         content. I can only provide general information..."
```

**LLMs are trained on general knowledge, not YOUR content.**

---

## The Solution: RAG (Retrieval-Augmented Generation)

**RAG = Give the LLM relevant context at query time**

```
┌─────────────────────────────────────────────────────────────┐
│                    RAG Pipeline                              │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  User Query: "What did SavageGeese say about the Crown?"     │
│       │                                                      │
│       ↓                                                      │
│  ┌─────────────┐                                             │
│  │  RETRIEVER  │ ← Search your indexed content               │
│  └─────────────┘                                             │
│       │                                                      │
│       ↓                                                      │
│  Retrieved Context:                                          │
│  "The Crown rides on a new TNGA-K platform... the ride       │
│   quality is surprisingly smooth for a sport sedan..."       │
│       │                                                      │
│       ↓                                                      │
│  ┌─────────────┐                                             │
│  │     LLM     │ ← Generate answer using retrieved context   │
│  └─────────────┘                                             │
│       │                                                      │
│       ↓                                                      │
│  Answer: "SavageGeese noted that the Toyota Crown has        │
│          surprisingly smooth ride quality for a sport        │
│          sedan, thanks to its TNGA-K platform."              │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

---

## RAG in yt-rag: A Concrete Example

**Step 1: User asks a question**
```bash
yt-rag ask "What cars have good fuel economy?"
```

**Step 2: System retrieves relevant sections**
```
Found 5 relevant sections:
1. "2024 Toyota Prius" @ 3:45 - "...achieves 57 mpg combined..."
2. "Honda Accord Hybrid" @ 12:30 - "...fuel efficiency is exceptional..."
3. "Mazda CX-50" @ 8:15 - "...surprisingly good gas mileage..."
```

**Step 3: LLM generates grounded answer**
```
Based on the videos in your library:
- The 2024 Toyota Prius leads with 57 mpg combined
- Honda Accord Hybrid offers exceptional efficiency
- Mazda CX-50 provides good mileage for an SUV

Sources: [links with timestamps]
```

---

## Why RAG Works: Grounding Prevents Hallucination

| Without RAG | With RAG |
|-------------|----------|
| LLM invents facts | LLM cites actual content |
| "The Prius gets about 50 mpg" | "The Prius achieves 57 mpg combined" (from video @ 3:45) |
| No sources | Clickable timestamp links |
| Generic answers | Specific to YOUR video library |

**Key insight:** The LLM becomes a "smart summarizer" of YOUR content, not a knowledge source.

---

## The RAG Formula

```
Answer = LLM( Query + Retrieved_Context )
```

**Quality depends on:**
1. **Retrieval quality** - Did we find the right content?
2. **Context formatting** - Did we present it well to the LLM?
3. **LLM capability** - Can it synthesize a good answer?

In practice: **Retrieval quality matters most!**

---

## Embeddings: The Foundation of Retrieval

**Embedding** = Converting text into a numerical vector

```
"fuel economy tips"  →  [0.12, -0.34, 0.87, ..., 0.23]
                            └─────── 1024 dimensions ─────┘
```

**Key Property:** Similar meanings → vectors that are close together

```
"fuel economy"      ·──────────────┐
"gas mileage"       ·──────────────┤  close in vector space
"mpg"               ·──────────────┘

"engine horsepower" ·───────────────────────── far away
```

**Why 1024 dimensions?** More dimensions = more nuance, but more compute.

---

## Vector Similarity: How Search Works

**Cosine Similarity** measures angle between vectors (0 to 1):

```
Query: "What cars get good gas mileage?"
        ↓ embed
       [0.23, -0.12, 0.45, ...]

Section A: "The Prius achieves 57 mpg..."     → similarity: 0.89 ✓
Section B: "The V8 produces 450 horsepower"   → similarity: 0.34
Section C: "Fuel efficiency is exceptional"   → similarity: 0.85 ✓
```

Higher similarity = more relevant content.

---

## What is FAISS?

**FAISS** = Facebook AI Similarity Search

A library for efficient similarity search of dense vectors.

```
Your video library: 10,000 sections
Each section: 1024-dimensional vector
Total: 10,000 × 1024 = 10M floats (~40 MB)

Brute force search: Compare query to all 10,000 vectors
FAISS IndexFlatIP: Same result, but optimized → 17ms
```

**Why FAISS?**
- Handles millions of vectors
- CPU-optimized (no GPU needed for our scale)
- Battle-tested by Facebook/Meta

---

## How Embedding Search Works in yt-rag

**Indexing (done once during `yt-rag embed`):**
```python
# For each video section in your library:
section_text = "Suspension: The Crown uses a multi-link rear setup..."
embedding = embed(section_text)  # → [0.12, -0.34, ...] (1024 dims)
faiss_index.add(embedding)       # Store in FAISS index
```

**Querying (per user question):**
```python
query = "What suspension does the Crown have?"
query_embedding = embed(query)   # → [0.08, -0.31, ...]

# Find closest vectors by cosine similarity
results = faiss_index.search(query_embedding, top_k=10)
# Returns: sections about Crown suspension, ranked by similarity
```

---

## Putting It All Together: yt-rag Query Flow

```
┌─────────────────────────────────────────────────────────────────────┐
│            yt-rag ask "What cars have a V8 engine?"                  │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│  1. EMBED QUERY                                                      │
│     "What cars have a V8 engine?" → [0.23, -0.45, 0.12, ...]        │
│                                                                      │
│  2. VECTOR SEARCH (FAISS)                                            │
│     Find top 50 sections with similar embeddings                     │
│     Score: cosine similarity (0.0 to 1.0)                            │
│                                                                      │
│  3. HYBRID RERANKING                                                 │
│     Boost results containing "V8" keyword                            │
│     Filter by query type (ENTITY search)                             │
│     → Top 5 most relevant sections                                   │
│                                                                      │
│  4. FORMAT CONTEXT                                                   │
│     "Section 1: Corvette C8 ZR1 - The 5.5L V8 produces..."          │
│     "Section 2: Ford Mustang GT - The Coyote V8..."                  │
│                                                                      │
│  5. LLM GENERATION                                                   │
│     Prompt: "Based on these video sections, answer: ..."             │
│     → Streamed answer with source citations                          │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

[↑ Back to Agenda](#agenda)

---

# Part 2: System Architecture

---

## yt-rag Overview

A complete RAG pipeline for YouTube videos:

- **Extract** transcripts from YouTube channels
- **Process** into semantic sections
- **Index** with vector embeddings
- **Search** with hybrid retrieval
- **Answer** questions with source citations

---

## High-Level Architecture

```
┌────────────────────────────────────────────────────────────────────┐
│                         yt-rag Architecture                         │
├────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  ┌─────────────┐    ┌─────────────┐    ┌─────────────┐             │
│  │   YouTube   │───→│  Ingestion  │───→│   SQLite    │             │
│  │   Videos    │    │   Pipeline  │    │   Database  │             │
│  └─────────────┘    └─────────────┘    └─────────────┘             │
│                                               │                     │
│                                               ↓                     │
│  ┌─────────────┐    ┌─────────────┐    ┌─────────────┐             │
│  │    User     │←──│    RAG      │←──│    FAISS    │             │
│  │   Query     │───→│   Service   │───→│   Index     │             │
│  └─────────────┘    └─────────────┘    └─────────────┘             │
│                           │                                         │
│                           ↓                                         │
│                    ┌─────────────┐                                  │
│                    │   Ollama/   │                                  │
│                    │   OpenAI    │                                  │
│                    └─────────────┘                                  │
│                                                                     │
└────────────────────────────────────────────────────────────────────┘
```

---

## Technology Stack

| Component | Technology | Why |
|-----------|------------|-----|
| CLI Framework | **Typer + Rich** | Beautiful terminal UX with progress bars |
| Data Models | **Pydantic v2** | Type safety, validation, serialization |
| Vector Store | **FAISS** | Facebook's fast similarity search (CPU) |
| Local LLM | **Ollama** | Privacy, no API costs, runs locally |
| Cloud LLM | **OpenAI** | Higher quality, pay-per-use option |
| Database | **SQLite** | Zero-config, single file, reliable |
| Package Manager | **uv** | 10-100x faster than pip |

[↑ Back to Agenda](#agenda)

---

# Part 3: The Data Pipeline

---

## Pipeline Overview

```
┌─────────────────────────────────────────────────────────────────────┐
│                      6-Step Ingestion Pipeline                       │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│  1. ADD          Add YouTube channel/video URL                       │
│       ↓                                                              │
│  2. SYNC         Discover all videos from channel                    │
│       ↓                                                              │
│  3. REFRESH      Fetch video metadata (title, duration, chapters)    │
│       ↓                                                              │
│  4. FETCH        Download transcripts (auto-captions)                │
│       ↓                                                              │
│  5. PROCESS      Sectionize + Summarize with LLM                     │
│       ↓                                                              │
│  6. EMBED        Generate vector embeddings                          │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

Run all steps with: `yt-rag update`

---

## Step 4-5: The Interesting Parts

### Transcript Fetching
```python
# Using youtube-transcript-api
transcript = YouTubeTranscriptApi.get_transcript(video_id)
# Returns: [{"text": "Hello", "start": 0.0, "duration": 2.5}, ...]
```

### Sectionizing Strategy
```
IF video has YouTube chapters:
    Use chapter boundaries (human-curated!)
ELSE:
    Use optimal time-based chunks:
    - Short videos (<10 min): 1.7 min chunks
    - Medium videos (<30 min): 3.1 min chunks
    - Long videos (30+ min): 4.5 min chunks
```

*Chunk sizes derived from analysis of 1000+ human-labeled chapters*

---

## Step 6: Building the Vector Index

```python
# For each section:
text = f"{section.title}\n\n{section.content}"
embedding = ollama_embed_text(text, model="mxbai-embed-large")
# Result: 1024-dimensional vector

# Store in FAISS index
index.add(np.array([embedding]))
```

**Storage:**
```
~/.yt-rag/
├── faiss_local/
│   ├── sections.index      # FAISS binary index
│   └── sections_meta.jsonl # Section metadata
└── faiss/                  # OpenAI embeddings (1536 dims)
```

[↑ Back to Agenda](#agenda)

---

# Part 4: Hybrid Search

---

## The Query Pipeline

```
┌─────────────────────────────────────────────────────────────────────┐
│                         Query Pipeline                               │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│  "What cars have good fuel economy?"                                 │
│       │                                                              │
│       ├──→ [Query Analysis] ← LLM classifies intent                  │
│       │         │                                                    │
│       │         ↓                                                    │
│       │    type: TOPIC                                               │
│       │    keywords: ["fuel", "economy", "mpg"]                      │
│       │                                                              │
│       └──→ [Embed Query] → [0.12, -0.34, ...]                        │
│                 │                                                    │
│                 ↓                                                    │
│       [FAISS Search] → Top 50 candidates                             │
│                 │                                                    │
│                 ↓                                                    │
│       [Keyword Filter + Rerank] → Top 10 final                       │
│                 │                                                    │
│                 ↓                                                    │
│       [LLM Answer Generation] with context                           │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

---

## Query Classification

8 query types for optimal retrieval:

| Type | Example | Strategy |
|------|---------|----------|
| **ENTITY** | "BMW M3" | Strict keyword matching |
| **TOPIC** | "fuel economy" | Semantic + synonyms |
| **COMPARISON** | "Camry vs Accord" | Multi-entity retrieval |
| **LIST** | "How many videos about EVs?" | Count from summaries |
| **FACTUAL** | "What is horsepower?" | Standard semantic |
| **FOLLOWUP** | "Tell me more" | Reuse previous hits |
| **POPULARITY** | "Most popular videos?" | Sort by view_count |
| **META** | "What channels do you have?" | Return library stats |

---

## Why Hybrid Search?

**Problem with pure semantic search:**
```
Query: "Model 3 review"

Pure Semantic Results:
1. Tesla Model S review   (similar meaning!)
2. Tesla Model Y review   (similar meaning!)
3. Model 3 review         (what we wanted)
```

**Solution:** Combine semantic + keyword matching

---

## Hybrid Scoring Formula

```python
# Keyword boost values
KEYWORD_BOOST_VIDEO_TITLE = 0.35
KEYWORD_BOOST_SECTION_TITLE = 0.20
KEYWORD_BOOST_SECTION_CONTENT = 0.10

# For ENTITY queries: penalize no keyword match
if query_type == ENTITY and keyword_matches == 0:
    relevance_multiplier = 0.3  # Heavy penalty

# Final score
final_score = (semantic_score * relevance_multiplier) + additive_boost
```

Result: "Model 3" query → Model 3 content ranks #1

---

## Synonym Expansion

Built-in domain synonyms improve recall:

```python
DEFAULT_SYNONYMS = {
    "mpg": ["fuel", "efficiency", "economy", "mileage"],
    "hp": ["horsepower", "power", "engine"],
    "awd": ["all wheel drive", "4wd", "four wheel"],
    "ev": ["electric", "battery", "bev"],
    ...
}
```

+ LLM-generated synonyms with approval workflow

[↑ Back to Agenda](#agenda)

---

# Part 5: Design Decisions & Tradeoffs

---

## Decision 1: Dual Backend (Ollama vs OpenAI)

**What we chose:** Support both local and cloud embeddings

```
~/.yt-rag/
├── faiss_local/  # Ollama: mxbai-embed-large (1024 dims)
└── faiss/        # OpenAI: text-embedding-3-small (1536 dims)
```

| | Ollama (Local) | OpenAI (Cloud) |
|---|---|---|
| **Cost** | Free | ~$0.02/1K tokens |
| **Privacy** | Data stays local | Sent to API |
| **Quality** | Good | Better |
| **Speed** | ~50ms/embed | ~100ms/embed |
| **Setup** | Install Ollama | Just API key |

**Tradeoff:** More storage, but users can choose per use-case

---

## Decision 2: Chapter-Based Sectionizing

**What we chose:** Use YouTube chapters when available, else time-chunks

**Why YouTube chapters?**
- Created by video authors (human-labeled!)
- Semantically meaningful boundaries
- Natural topic divisions

**Fallback chunking sizes** (empirically derived):
- Short (<10 min): 1.7 min chunks
- Medium (<30 min): 3.1 min chunks
- Long (30+ min): 4.5 min chunks

**Tradeoff:** Time-based chunks may split mid-topic

---

## Decision 3: Two-Phase Retrieval

**What we chose:** Search both summaries AND sections

```
Phase 1: Search VIDEO SUMMARIES
    → Get overview of matching videos
    → Enable "how many videos about X?" queries

Phase 2: Search SECTIONS
    → Get detailed passages
    → Precise timestamp linking
```

**Tradeoff:** Extra index to maintain, but enables richer queries

---

## Decision 4: Query Analysis via LLM

**What we chose:** LLM classifies query intent before retrieval

```python
# Extra LLM call per query (~200ms)
analysis = await analyze_query_with_llm(query)
# Returns: type, keywords, time_filter
```

**Benefits:**
- Optimal retrieval strategy per query type
- Keyword extraction for hybrid search
- Detection of follow-up questions

**Tradeoff:** Added latency, but much better precision

---

## Decision 5: Streaming Responses

**What we chose:** Stream LLM output token-by-token

```python
async for chunk in aollama_chat_stream(messages):
    yield chunk  # Display immediately
```

**Why?**
- First token in ~200ms vs 2s for full response
- Much better UX for interactive chat
- Same total time, feels faster

**Tradeoff:** More complex async code

---

## Decision 6: CPU-Only FAISS (v0.2.0)

**What we chose:** Use `faiss-cpu` only (removed GPU support in v0.2.0)

**Why?**
- `faiss-gpu-cu12` incompatible with CUDA 13
- Most users don't have NVIDIA GPUs
- Search is fast enough on CPU (~17ms)
- Ollama still uses GPU for LLM inference

**Removed in v0.2.0:** ~540 lines of GPU configuration code

**Tradeoff:** No GPU-accelerated search, but simpler codebase

[↑ Back to Agenda](#agenda)

---

# Part 6: Benchmarking

---

## Test Generation Pipeline

Automated 3-step test case generation:

```
┌─────────────────────────────────────────────────────────────────────┐
│                    Test Generation Pipeline                          │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│  Step 1: PREPARE                                                     │
│    Sample 15 videos per channel                                      │
│    Extract sections (truncated to 2000 chars)                        │
│    → raw_videos.json                                                 │
│                                                                      │
│  Step 2: ANALYZE                                                     │
│    LLM extracts from each video:                                     │
│    - Entities (brands, models, people)                               │
│    - Topics (themes discussed)                                       │
│    - Facts (claims with numbers)                                     │
│    - Comparisons (X vs Y)                                            │
│    → video_analysis_gpt-4o.json                                      │
│                                                                      │
│  Step 3: BUILD                                                       │
│    Generate balanced test queries:                                   │
│    - 5 entity queries per channel                                    │
│    - 5 topic queries per channel                                     │
│    - 5 comparison queries per channel                                │
│    - 5 list/count queries per channel                                │
│    - Global META queries                                             │
│    → benchmark_generated_gpt-4o.json                                 │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

---

## Test Case Structure

```json
{
  "query": "Chevy",
  "expected_type": "entity",
  "expected_keywords": ["chevy"],
  "expected_video_ids": ["raaM2zcpAoM"],
  "channel": "savagegeese",
  "source_video": "Cars, Trucks, and Debt | Why it Never Ends"
}
```

**Coverage:** 163 test cases across 8 channels

---

## Evaluation Metrics

### Retrieval Quality
- **Precision@K** - Relevant results in top K / K
- **Recall** - Found / Total expected
- **MRR** - 1 / rank of first relevant result

### Answer Quality
- **Keyword Match** - Expected keywords in answer
- **LLM Validation** - LLM judges answer relevance

### Pass/Fail Criteria
```python
if expected_video_ids and precision_at_k < 0.2:
    passed = False
if expected_keywords and keyword_match < 0.5:
    passed = False
```

---

## Benchmark Results (v0.2.0)

| Metric | Value | Notes |
|--------|-------|-------|
| **Total Tests** | 160 | Across 8 YouTube channels |
| **Query Classification Accuracy** | 57.5% | Room for improvement |
| **Keyword Match Rate** | 81.0% | Retrieval works well |
| **LLM Validation Pass Rate** | 63.1% | GPT-4o-mini validator |

### Timing Breakdown (avg per query)
| Stage | Time | % of Total |
|-------|------|------------|
| Parallel (embed + analyze) | 497ms | 13% |
| FAISS Search | 25ms | <1% |
| Keyword Filtering | 2ms | <1% |
| Answer Generation | 1,380ms | 35% |
| Validation | 2,033ms | 52% |
| **Total** | **3,939ms** | 100% |

**Insight:** Validation dominates (GPT-4o API calls). FAISS search is blazing fast (~25ms).

---

## Example Passing Tests

```
Query: "Chevy"
Type: entity ✓
Keywords Found: ["chevy"] ✓
Answer: "Videos available include reviews of the 2020
        Chevy Silverado Trail Boss LT, the 2015 Chevy
        Silverado Custom, and the 2021 Chevy Silverado
        Diesel."
Validation: PASS ✓
```

```
Query: "Suspension"
Type: topic ✓
Keywords Found: ["suspension"] ✓
Answer: "The videos discuss various aspects of suspension
        systems, including MacPherson strut and multi-link
        setups in different vehicles..."
Validation: PASS ✓
```

---

## Example Failing Tests

```
Query: "Cody" (person's name)
Expected Type: entity
Got Type: topic ✗
Answer: "The video discusses Codycoin, an altcoin..."
Validation: FAIL - Wrong entity!
```

**Why it failed:** Query classification confused person name with crypto

```
Query: "V8 preference"
Keywords Missing: ["preference"]
Answer: "Videos discussing V8 engines include..."
Validation: FAIL - Didn't address preference
```

**Why it failed:** Answer listed V8 videos but didn't discuss preferences

---

## Key Insights from Benchmarking

### What Works Well
- Entity queries with unique names (brands, models)
- Topic queries with clear keywords
- Hybrid search significantly improves precision

### What Needs Improvement
- Query classification (59.5% accuracy)
- Abstract queries ("preference", "concerns")
- Ambiguous entity names (person vs product)

### The 81% vs 63% Gap
- **81%** keyword match = retrieval finds relevant content
- **63%** GPT-4o validation = answer generation is decent
- Retrieval still outperforms generation

[↑ Back to Agenda](#agenda)

---

# Summary

---

## What We Built

A production-quality RAG pipeline demonstrating:

1. **End-to-end architecture** - From YouTube to answers
2. **Hybrid search** - Semantic + keyword for precision
3. **Query understanding** - LLM-powered intent classification
4. **Dual backends** - Local (Ollama) and cloud (OpenAI)
5. **Automated testing** - LLM-generated benchmarks
6. **Real metrics** - Honest evaluation of limitations

---

## Key Takeaways

1. **RAG grounds LLM responses** - No more hallucination about your content
2. **Hybrid search > pure semantic** - Keywords matter for precision
3. **Query classification pays off** - Different queries need different strategies
4. **Evaluation is hard** - Even LLM validators disagree
5. **Streaming UX matters** - First token in 200ms vs waiting 2s
6. **Fix retrieval first** - 81% retrieval vs 63% generation success

**The biggest insight:** If retrieval finds the right content, the LLM usually generates a good answer. Fix retrieval first.

---

## Try It Yourself (v0.2.0)

```bash
# Install (requires Python 3.14 and Ollama)
pip install yt-rag

# Add your favorite YouTube channel
yt-rag add https://www.youtube.com/@3blue1brown

# Run the full pipeline (sync, fetch, process, embed)
yt-rag update

# Ask questions about your videos
yt-rag ask "What is the intuition behind neural networks?"

# Or use interactive chat with follow-up questions
yt-rag chat
```

**Requirements:**
- Python 3.14+
- Ollama (for local mode) or OpenAI API key (for cloud mode)

---

## Q&A

Questions?

---

## Thank You

**yt-rag v0.2.0** - A RAG Pipeline for YouTube Content

- Hybrid search (semantic + keyword)
- Dual backends (Ollama / OpenAI)
- Automated benchmarking
- ~2.5s end-to-end query latency

**What's next for v0.3.0?**
- Improve query classification (target: 80%+)
- Better answer generation prompts
- Web UI for non-CLI users
- Cross-encoder reranking

[↑ Back to Agenda](#agenda)

---

# Appendix

---

## A1: Full Query Pipeline Code Flow

```
cli.py:ask_command()
    ↓
service.py:RAGService.ask_stream()
    ├─→ search.py:analyze_query_with_llm()  [parallel]
    └─→ openai_client.py:embed_text()        [parallel]
    ↓
vectorstore.py:VectorStore.search()
    ↓
search.py:compute_relevance_metrics()
    ↓
service.py:_format_context()
    ↓
openai_client.py:aollama_chat_stream()
```

---

## A2: Database Schema (Key Tables)

```sql
CREATE TABLE videos (
    id TEXT PRIMARY KEY,
    channel_id TEXT,
    title TEXT,
    description TEXT,
    duration INTEGER,
    view_count INTEGER,
    transcript_status TEXT,  -- 'pending', 'success', 'failed'
    processed_at TIMESTAMP
);

CREATE TABLE sections (
    id TEXT PRIMARY KEY,
    video_id TEXT,
    title TEXT,
    content TEXT,
    start_time REAL,
    end_time REAL,
    embedded INTEGER DEFAULT 0
);

CREATE TABLE summaries (
    video_id TEXT PRIMARY KEY,
    summary TEXT,
    topics TEXT,  -- JSON array
    entities TEXT,  -- JSON array
    embedded INTEGER DEFAULT 0
);
```

---

## A3: Embedding Dimensions

| Model | Provider | Dimensions |
|-------|----------|------------|
| mxbai-embed-large | Ollama | 1024 |
| text-embedding-3-small | OpenAI | 1536 |
| text-embedding-3-large | OpenAI | 3072 |

FAISS index must match embedding dimension exactly.

---

## A4: Performance Characteristics

| Operation | Time | Notes |
|-----------|------|-------|
| Embed 1 text | 50ms | Ollama local |
| Embed batch (100) | 2s | Ollama local |
| FAISS search | 17ms | 10K vectors, CPU |
| Query analysis | 200ms | 7B model |
| Answer generation | 1.3s | Streaming, first token ~200ms |
| Full ask query | 2.5s | End-to-end |

---

## A5: Makefile Commands

```bash
make install       # uv sync
make lint          # ruff format + check --fix
make build         # Full update pipeline
make build-test    # Test mode (5 videos/channel)
make test          # Run benchmark
make test-openai   # Benchmark with OpenAI
make test-report   # Generate HTML report
make status        # Show database stats
```

[↑ Back to Agenda](#agenda)
